{{- if .Values.observability.prometheus.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "trustlayer.fullname" . }}-alerts
  labels:
    {{- include "trustlayer.labels" . | nindent 4 }}
    prometheus: kube-prometheus
spec:
  groups:
    - name: trustlayer.availability
      interval: 30s
      rules:
        - alert: TrustLayerFrontendDown
          expr: up{job="{{ include "trustlayer.fullname" . }}-frontend"} == 0
          for: 5m
          labels:
            severity: critical
            component: frontend
          annotations:
            summary: "TrustLayer frontend is down"
            description: "Frontend pod {{ $labels.pod }} has been down for more than 5 minutes."

        - alert: TrustLayerHighErrorRate
          expr: |
            sum(rate(http_requests_total{job="{{ include "trustlayer.fullname" . }}-frontend",status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job="{{ include "trustlayer.fullname" . }}-frontend"}[5m]))
            > 0.02
          for: 5m
          labels:
            severity: warning
            component: frontend
          annotations:
            summary: "High error rate detected"
            description: "Error rate is {{ $value | humanizePercentage }} (threshold: 2%)"

        - alert: TrustLayerHighLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket{job="{{ include "trustlayer.fullname" . }}-frontend"}[5m])) by (le)
            ) > 2.0
          for: 10m
          labels:
            severity: warning
            component: frontend
          annotations:
            summary: "High latency detected"
            description: "P95 latency is {{ $value }}s (SLO: 2.0s)"

    - name: trustlayer.database
      interval: 30s
      rules:
        - alert: PostgreSQLDown
          expr: pg_up{job="{{ include "trustlayer.fullname" . }}-postgresql"} == 0
          for: 5m
          labels:
            severity: critical
            component: postgresql
          annotations:
            summary: "PostgreSQL is down"
            description: "PostgreSQL instance {{ $labels.instance }} is down."

        - alert: PostgreSQLHighConnections
          expr: |
            sum(pg_stat_database_numbackends{job="{{ include "trustlayer.fullname" . }}-postgresql"})
            /
            pg_settings_max_connections{job="{{ include "trustlayer.fullname" . }}-postgresql"}
            > 0.8
          for: 5m
          labels:
            severity: warning
            component: postgresql
          annotations:
            summary: "PostgreSQL connection pool is 80% full"
            description: "Connection usage is {{ $value | humanizePercentage }}"

        - alert: PostgreSQLSlowQueries
          expr: |
            pg_stat_activity_max_tx_duration{job="{{ include "trustlayer.fullname" . }}-postgresql"} > 60
          for: 5m
          labels:
            severity: warning
            component: postgresql
          annotations:
            summary: "Slow queries detected"
            description: "Query running for {{ $value }}s on database {{ $labels.datname }}"

        - alert: PostgreSQLReplicationLag
          expr: |
            pg_replication_lag{job="{{ include "trustlayer.fullname" . }}-postgresql"} > 10
          for: 5m
          labels:
            severity: warning
            component: postgresql
          annotations:
            summary: "PostgreSQL replication lag detected"
            description: "Replication lag is {{ $value }}s on replica {{ $labels.instance }}"

    - name: trustlayer.backup
      interval: 1h
      rules:
        - alert: TrustLayerBackupFailed
          expr: |
            time() - trustlayer_last_successful_backup_timestamp > 86400 * 2
          for: 1h
          labels:
            severity: critical
            component: backup
          annotations:
            summary: "Database backup has failed"
            description: "No successful backup in the last 48 hours."

    - name: trustlayer.resources
      interval: 30s
      rules:
        - alert: PodMemoryUsageHigh
          expr: |
            sum(container_memory_working_set_bytes{namespace="{{ .Release.Namespace }}",pod=~"{{ include "trustlayer.fullname" . }}.*"})
            by (pod)
            /
            sum(container_spec_memory_limit_bytes{namespace="{{ .Release.Namespace }}",pod=~"{{ include "trustlayer.fullname" . }}.*"})
            by (pod)
            > 0.9
          for: 5m
          labels:
            severity: warning
            component: resources
          annotations:
            summary: "Pod memory usage is high"
            description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"

        - alert: PodCPUUsageHigh
          expr: |
            sum(rate(container_cpu_usage_seconds_total{namespace="{{ .Release.Namespace }}",pod=~"{{ include "trustlayer.fullname" . }}.*"}[5m]))
            by (pod)
            /
            sum(container_spec_cpu_quota{namespace="{{ .Release.Namespace }}",pod=~"{{ include "trustlayer.fullname" . }}.*"} / 100000)
            by (pod)
            > 0.9
          for: 5m
          labels:
            severity: warning
            component: resources
          annotations:
            summary: "Pod CPU usage is high"
            description: "Pod {{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }}"

        - alert: PVCAlmostFull
          expr: |
            kubelet_volume_stats_used_bytes{namespace="{{ .Release.Namespace }}"}
            /
            kubelet_volume_stats_capacity_bytes{namespace="{{ .Release.Namespace }}"}
            > 0.85
          for: 10m
          labels:
            severity: warning
            component: storage
          annotations:
            summary: "PVC is almost full"
            description: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full"
{{- end }}
